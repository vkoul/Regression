{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "ml1-arm64",
      "language": "python",
      "name": "ml1-arm64"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "nteract": {
      "version": "0.7.1"
    },
    "colab": {
      "name": "linreg.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-qjsO3-CyTH"
      },
      "source": [
        "# Linear Regression Multiple Ways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhI2fAh1C360"
      },
      "source": [
        "!git init; git pull https://github.com/YOUR_USERNAME/Regression.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBBWZsb3CyTO"
      },
      "source": [
        "## Making the data\n",
        "\n",
        "We'll first construct a synthetic data set..using a function from the `scikit-learn` library. Synthetic data is nice in the sense that we can constrain how the noise behaves, and thus isolate effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDBQLvGDCyTP"
      },
      "source": [
        "%matplotlib inline\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoCW2nSyCyTR"
      },
      "source": [
        "This data is generated from the canonical generating process assumed for linear regression: a gaussian distribution centered at the regression line on the y axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4g4sN9CyTR"
      },
      "source": [
        "#code adapted from http://tillbergmann.com/blog/python-gradient-descent.html\n",
        "X, y, coef = make_regression(n_samples = 100, \n",
        "                       n_features=1, \n",
        "                       noise=20,\n",
        "                       random_state=2017,\n",
        "                       bias=0.0,\n",
        "                       coef=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98EPmtRCyTS"
      },
      "source": [
        "Notice that the X is in the canonical array-of-arrays format.\n",
        "**Try and print its shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNpyFGTeCyTS"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij_miKEiCyTU"
      },
      "source": [
        "We are fitting a model with an intercept. Lets see what it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5STse-DGCyTU"
      },
      "source": [
        "coef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRSyktEDCyTV"
      },
      "source": [
        "We can plot the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f34COrzrCyTV"
      },
      "source": [
        "plt.plot(X,y, 'o');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBwzhUNYCyTW"
      },
      "source": [
        "For the purposes of drawing the regression line, lets create a uniform grid of points, and then reshape it into the canonical format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmedjoqJCyTX"
      },
      "source": [
        "xgrid = np.linspace(-2.5,2.5,1000)\n",
        "Xgrid = xgrid.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qx4TklhCyTY"
      },
      "source": [
        "## Fit using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecdZyQdpCyTY"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN8FEHXqCyTY"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X,y) # fit the model with the existing data\n",
        "ypgrid = lr.predict(Xgrid) # now predict it on the grid\n",
        "lr.coef_, lr.intercept_ # get the slope and the intercept"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihlAru1ACyTY"
      },
      "source": [
        "Notice that the slope and the intercept are not what we fed into the model, but close. This is because the model fitted depends on the exact way points were generated.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xThEkz-FCyTZ"
      },
      "source": [
        "plt.plot(Xgrid, ypgrid)\n",
        "plt.plot(X, y, '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vs9Qhf2CyTa"
      },
      "source": [
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGmhfmAjCyTa"
      },
      "source": [
        "r2_score(y, lr.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7g78BarCyTb"
      },
      "source": [
        "## The impact of samples\n",
        "\n",
        "We'll sample 20 points from the data set. We do this by sampling 20 indices, index into X and y, and then fit on the sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ajLCrOWCyTb"
      },
      "source": [
        "sample_indices = np.random.choice(range(100), size=20, replace=False)\n",
        "sample_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qjIde5_CyTb"
      },
      "source": [
        "We create a sample by using the sample indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3fp5XwTCyTc"
      },
      "source": [
        "Xsample = X[sample_indices]\n",
        "ysample = y[sample_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE3G5P4iCyTc"
      },
      "source": [
        "**Find the $R^2$ score of a fit to this sample, on this sample**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8M1Da1CyTc"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKo99KksCyTd"
      },
      "source": [
        "Lets check the sensitivity of our prediction to our sample. We'll do this 1000 times..that is we'll sample a new set of 20 points, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyD960zACyTe"
      },
      "source": [
        "scores = []\n",
        "models=[]\n",
        "for i in range(1000):\n",
        "    sample_indices = np.random.choice(range(100), size=20, replace=False)\n",
        "    Xsample = X[sample_indices]\n",
        "    ysample = y[sample_indices]\n",
        "    m = LinearRegression().fit(Xsample, ysample)\n",
        "    models.append(m)\n",
        "    scores.append(m.score(Xsample, ysample))\n",
        "plt.hist(scores,  bins=np.linspace(0.7, 1, 30))\n",
        "plt.xlim(0.7,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAqEUbtvCyTe"
      },
      "source": [
        "Let us check the slope and intercepts fitted on the different samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMn-JQLkCyTe"
      },
      "source": [
        "plt.hist([models[i].coef_[0] for i in range(1000)], bins=10);\n",
        "plt.xlim([60, 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZnVAyICyTf"
      },
      "source": [
        "plt.hist([models[i].intercept_ for i in range(100)], bins=10);\n",
        "plt.xlim([-15, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D5cIb07CyTf"
      },
      "source": [
        "## The impact of noise\n",
        "\n",
        "**Redo this with a higher amount of noise (about 400)**. For this you will need to create a new dataset. Plot the data. Plot the histogram of the R^2 as well as that of the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrW_wcIkCyTf"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq-nZKfDCyTg"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNcQhOJnCyTg"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fr4EJraCyTg"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IkDhVVnCyTg"
      },
      "source": [
        "## Impact of sample size\n",
        "\n",
        "Going back to the original dataset with less noise, now fetch smaller size samples  (10 data points each) and repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSHJRIJkCyTh"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO7MH8z3CyTh"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn0zwqBuCyTh"
      },
      "source": [
        "plt.hist([models[i].intercept_ for i in range(100)], bins=10);\n",
        "plt.xlim([-15, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeErYPb9CyTi"
      },
      "source": [
        "## Testing and training\n",
        "\n",
        "A grid like the one we created might contain some of the points we fit this model on. This is called **Data Contamination** and is a big no-no. If we want an independent estimate of the error, we should hold out some points in a test set. Then we want to guarantee that there is no overlap between the initial sample, or **training set**, and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJH20BmCyTi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBRKTAcGCyTi"
      },
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=2017)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASm-rgK_CyTi"
      },
      "source": [
        "Now lets fit the model on the training set and evaluate it both on the training set and the test set. We print the R^2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IqnGc1iCyTi"
      },
      "source": [
        "lr2 = LinearRegression().fit(Xtrain, ytrain)\n",
        "r2_test = r2_score(ytest, lr.predict(Xtest))\n",
        "r2_train = r2_score(ytrain, lr.predict(Xtrain))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_tnvbNfCyTj"
      },
      "source": [
        "\"Train R2 is {}, while test R^2 is {}\".format(r2_train, r2_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bRVGa0ACyTj"
      },
      "source": [
        "## Using Keras to fit the model\n",
        "\n",
        "We'll use SGD (we could have used plain and simple gradient descent, why?) and Keras's Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQsuCh9hCyTj"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "lr3 = Sequential()\n",
        "lr3.add(Dense(1, input_shape=(1,)))\n",
        "lr3.compile(optimizer='sgd', loss='mean_squared_error',  metrics=['mse','accuracy'])\n",
        "lr3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2miJgYFLCyTk"
      },
      "source": [
        "history = lr3.fit(Xtrain, ytrain, epochs=100, batch_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAhz39NZCyTk"
      },
      "source": [
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qId7nfpLCyTm"
      },
      "source": [
        "lr3.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}